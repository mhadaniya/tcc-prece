\chapter{CONCLUSÃO}

\indent Este trabalho apresentou um estudo sobre a detecção de anomalias em redes de computadores, as principais abordagens e algoritmos utilizados para a detecção de anomalias. Foram relatadas as particularidades, vantagens e desvantagens de cada técnica, além do modo com que cada abordagem lida com o problema da detecção de dados anômalos em um ambiente de tráfego da rede, que é altamente heterogêneo.

\indent Foi utilizado o algoritmo K-means, que através do cálculo da distância Euclideana, verifica a semelhança e realiza o agrupamento os dados. O algoritmo foi aplicado na base de dados KDDcup99, onde, pôde-se relatar os seus atributos e os tipos de tráfego que a base contém. Ademais, o trabalho relata todo o processo e os meios para a preparação e normalização da base de dados, e o algoritmo K-means++, que foi a metodologia utilizada para realizar a escolha dos centros iniciais.

\indent Foram realizados vários experimentos e validações para apresentar o funcionamento do algoritmo. Dentre eles, experimentos que apresentaram soluções ótimas e experimentos realizados em situações totalmente desfavoráveis ao agrupamento, com intuito de relatar os melhores e piores casos de agrupamento e como o contexto do tráfego pode interferir sendo benéfico ou prejudicial para a tarefa de clusterização.

\indent Pôde-se demonstrar através dos resultados experimentais, que apesar da simplicidade do algoritmo, é possível utilizá-lo para a detecção de anomalias obtendo-se grande eficácia. Os problemas do algoritmo em relação à escolha dos centros inicias foram resolvidos com a utilização do K-means++, onde nenhum grupo foi omitido durante a clusterização e os resultados apresentaram taxas de acertos que foram muito superiores aos erros.


\section{Trabalhos Futuros}

\indent Com os conhecimentos adquiridos e conclusões obtidas neste trabalho, pode-se sugerir como trabalho futuro, a implementação de uma solução de análise de séries temporais de tráfego, mesclando as técnicas de análises estatísticas do tráfego e redes neurais, como a \textit{Multi-Layer Perceptrons}. Onde, as bases de dados sejam retiradas do tráfego real, para que seja possível a obtenção de uma solução mais robusta e com possibilidade de aplicação em situações reais.
%% conteudo
